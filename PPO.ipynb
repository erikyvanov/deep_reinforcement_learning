{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b651199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from jaxtyping import Float, Int64\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from typing import cast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67894fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(layer: nn.Module, std= np.sqrt(2), bias_const: float = 0.0) -> nn.Module:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNewtwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            init_layers(nn.Linear(state_dim, hidden_dim), std=np.sqrt(2)),\n",
    "            nn.ReLU(),\n",
    "            init_layers(nn.Linear(hidden_dim, hidden_dim), std=np.sqrt(2)),\n",
    "            nn.ReLU(),\n",
    "            init_layers(nn.Linear(hidden_dim, action_dim), std=0.01)\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739adf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            init_layers(nn.Linear(state_dim, hidden_dim), std=np.sqrt(2)),\n",
    "            nn.ReLU(),\n",
    "            init_layers(nn.Linear(hidden_dim, hidden_dim), std=np.sqrt(2)),\n",
    "            nn.ReLU(),\n",
    "            init_layers(nn.Linear(hidden_dim, 1), std=1.0)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23501ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # FASE 0: Architecture Definition\n",
    "        # Actor: π_θ(a_t|s_t): S → Δ(A)\n",
    "        self.actor = ActorNewtwork(state_dim, action_dim, hidden_dim)\n",
    "        # Critic: V_φ(s_t): S → R\n",
    "        self.critic = CriticNetwork(state_dim, hidden_dim)\n",
    "\n",
    "    def get_action_and_log_prob(self, state: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        action_logits = self.actor(state)\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_probability = action_distribution.log_prob(action)\n",
    "\n",
    "        return action, action_log_probability\n",
    "    \n",
    "    def get_action_log_prob_entropy(self, state: torch.Tensor, action) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        action_logits = self.actor(state)\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "        action_log_probability = action_distribution.log_prob(action)\n",
    "        # Fórmula 3.2.3: Entropy Bonus\n",
    "        # S[π_θ](s_t) = -Σ π_θ(a|s_t) log π_θ(a|s_t)\n",
    "        entropy = action_distribution.entropy()\n",
    "\n",
    "        return action, action_log_probability, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd73a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Notation Key: Mathematical Sets\n",
    "# S: State Space (espacio de observaciones)\n",
    "# A: Action Space (espacio de acciones discretas)\n",
    "# D: Dataset/Rollout Buffer (conjunto completo de transiciones recolectadas)\n",
    "# B: Minibatch (subconjunto aleatorio de D para un paso de gradiente)\n",
    "\n",
    "class RolloutBuffer(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            buffer_size: int,\n",
    "            state_dimension: int,\n",
    "            device: torch.device):\n",
    "        self._state_buffer: Float[Tensor, \"state_dimension\"] = torch.zeros(\n",
    "            (buffer_size, state_dimension)).to(device)\n",
    "        self._action_buffer: Int64[Tensor, \"\"] = torch.zeros(\n",
    "            (buffer_size), dtype=torch.int64).to(device)\n",
    "        self._log_probabilities: Float[Tensor, \"\"] = torch.zeros(\n",
    "            (buffer_size)).to(device)\n",
    "        self._rewards: Float[Tensor, \"\"] = torch.zeros(\n",
    "            (buffer_size)).to(device)\n",
    "        self._done_flags: Float[Tensor, \"\"] = torch.zeros(\n",
    "            (buffer_size)).to(device)\n",
    "        self._state_value_predictions: Float[Tensor, \"\"] = torch.zeros(\n",
    "            (buffer_size)).to(device)\n",
    "\n",
    "        self.reset()\n",
    "        self._buffer_size = buffer_size\n",
    "        self._device = device\n",
    "\n",
    "    # FASE 1: Almacenar transición τ_t = (s_t, a_t, log π_θ_old(a_t|s_t), r_t, d_t, V_φ_old(s_t))\n",
    "    def add(\n",
    "            self,\n",
    "            state: np.ndarray,\n",
    "            action: int,\n",
    "            logprob: float,\n",
    "            reward: float,\n",
    "            is_episode_done: bool,\n",
    "            state_value_prediction: float):\n",
    "        self._state_buffer[self._pos] = torch.as_tensor(\n",
    "            state, device=self._device)\n",
    "        self._action_buffer[self._pos] = action\n",
    "        self._log_probabilities[self._pos] = logprob\n",
    "        self._rewards[self._pos] = reward\n",
    "        self._done_flags[self._pos] = float(is_episode_done)\n",
    "        self._state_value_predictions[self._pos] = state_value_prediction\n",
    "\n",
    "        self._pos += 1\n",
    "\n",
    "    def compute_return_target(self, last_value, gamma, lamb):\n",
    "        # FASE 2: GAE (Generalized Advantage Estimation) - Backward Pass\n",
    "        self._gaes = torch.zeros((self._buffer_size)).to(self._device)\n",
    "\n",
    "        for t in reversed(range(self._buffer_size)):\n",
    "            is_last = t == self._buffer_size - 1\n",
    "            if is_last:\n",
    "                next_value = last_value  # Bootstrap: V_φ_old(s_{T+1})\n",
    "                next_gae = 0  # Inicialización: Â_{T+1} = 0\n",
    "            else:\n",
    "                next_value = self._state_value_predictions[t+1]\n",
    "                next_gae = self._gaes[t+1]\n",
    "\n",
    "            # Fórmula 2.1: TD Residual\n",
    "            # δ_t = r_t + γ·V_φ_old(s_{t+1})·(1-d_t) - V_φ_old(s_t)\n",
    "            delta_t = self._rewards[t] + gamma*next_value * \\\n",
    "                (1-self._done_flags[t]) - self._state_value_predictions[t]\n",
    "\n",
    "            # Fórmula 2.2: GAE Recursion\n",
    "            # Â_t = δ_t + (γλ)·(1-d_t)·Â_{t+1}\n",
    "            gae = delta_t + gamma*lamb*(1-self._done_flags[t])*next_gae\n",
    "            self._gaes[t] = gae\n",
    "\n",
    "        # Fórmula 2.3: Return Target\n",
    "        # R_t = Â_t + V_φ_old(s_t)\n",
    "        self._return_targets = self._gaes + self._state_value_predictions\n",
    "\n",
    "    @property\n",
    "    def gaes(self):\n",
    "        return self._gaes\n",
    "\n",
    "    @property\n",
    "    def return_targets(self):\n",
    "        return self._return_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._state_buffer.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return {\n",
    "            'state_buffer': self._state_buffer[idx],\n",
    "            'action_buffer': self._action_buffer[idx],\n",
    "            'log_probabilities': self._log_probabilities[idx],\n",
    "            'rewards': self._rewards[idx],\n",
    "            'done_flags': self._done_flags[idx],\n",
    "            'state_value_predictions': self._state_value_predictions[idx],\n",
    "            'return_targets': self._return_targets[idx],\n",
    "            'gaes': self._gaes[idx]\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        self._pos = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATIONS = 250\n",
    "ENV_NAME = 'LunarLander-v3'\n",
    "\n",
    "env = gym.make(ENV_NAME, gravity=-10.0,\n",
    "               enable_wind=True, wind_power=15.0, turbulence_power=1.5)\n",
    "\n",
    "action_dim = int(cast(Discrete, env.action_space).n)\n",
    "state_dim = cast(Box, env.observation_space).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e049c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 64\n",
    "BUFFER_SIZE = 1024\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "gamma = 0.99\n",
    "learning_rate = 3.0e-4\n",
    "epsilon = 0.1\n",
    "\n",
    "c1 = 0.5\n",
    "c2 = 0.01\n",
    "\n",
    "agent = PPOAgent(state_dim, action_dim, HIDDEN_DIM).to(device)\n",
    "buffer = RolloutBuffer(BUFFER_SIZE, state_dim, device)\n",
    "\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gae_lamb = 0.95\n",
    "clip_epsilon = 0.2\n",
    "max_norm_value = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "last_state = state\n",
    "\n",
    "avg_losses = []\n",
    "avg_entropies = []\n",
    "avg_rewards = []\n",
    "\n",
    "for iteration in range(NUM_ITERATIONS):\n",
    "    # FASE 1: ROLLOUT (Generación de Trayectorias)\n",
    "    # Recolectamos D = {τ_t}, donde τ_t = (s_t, a_t, log π_θ_old(a_t|s_t), r_t, d_t, V_φ_old(s_t))\n",
    "    # Usamos π_θ_old (frozen snapshot) para generar las trayectorias\n",
    "    state = last_state\n",
    "    for step in range(BUFFER_SIZE):\n",
    "        with torch.no_grad():  # No gradientes: solo recolección de datos\n",
    "            action, action_log_probability = agent.get_action_and_log_prob(torch.as_tensor(state, device=device))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(int(action.item()))\n",
    "            is_episode_done = terminated or truncated \n",
    "            state_value_prediction = agent.critic(torch.as_tensor(state, device=device))\n",
    "            # Almacenar: (s_t, a_t, log π_θ_old, r_t, d_t, V_φ_old)\n",
    "            buffer.add(state, int(action), float(action_log_probability), float(reward), is_episode_done, float(state_value_prediction))\n",
    "        if is_episode_done:\n",
    "            state, _ = env.reset()\n",
    "            last_state = state\n",
    "        else:\n",
    "            state = next_state\n",
    "            last_state = next_state\n",
    "\n",
    "    # Bootstrap: Obtener V_φ_old(s_{T+1}) para el último estado fuera del buffer\n",
    "    with torch.no_grad():\n",
    "        next_state_value_prediction = agent.critic(torch.as_tensor(state, device=device))\n",
    "\n",
    "    # FASE 2: GAE - Cálculo de ventajas y retornos (Backward Pass)\n",
    "    buffer.compute_return_target(next_state_value_prediction, gamma, gae_lamb)\n",
    "    \n",
    "    # Preparar DataLoader: Dividir D en minibatches B\n",
    "    train_loader = DataLoader(\n",
    "        buffer, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,  # Romper correlación temporal\n",
    "        generator=torch.Generator(device=device)\n",
    "    )\n",
    "    losses = []\n",
    "    entropies = []\n",
    "    \n",
    "    # FASE 3: OPTIMIZACIÓN PPO (K epochs sobre M minibatches)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch in train_loader:  # Iterar sobre cada minibatch B ⊂ D\n",
    "            state = batch['state_buffer']\n",
    "            gaes: torch.Tensor = batch['gaes']\n",
    "            old_log_probs = batch['log_probabilities']  # log π_θ_old (constante)\n",
    "            action = batch['action_buffer']\n",
    "\n",
    "            # Re-evaluación: Pasar estados por la red ACTUAL (π_θ, V_φ)\n",
    "            _, log_prob, entropy = agent.get_action_log_prob_entropy(state, action)\n",
    "\n",
    "            # Fórmula 3.1: Normalización de Ventajas (sobre el minibatch B)\n",
    "            # Â_{t,norm} = (Â_t - μ_B(Â)) / (σ_B(Â) + ε_stab)\n",
    "            advatages_norm = (gaes - gaes.mean()) / (gaes.std() + epsilon)\n",
    "            \n",
    "            # Fórmula 3.2.1: Ratio de Probabilidad (Importance Sampling)\n",
    "            # r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) = exp(log π_θ - log π_θ_old)\n",
    "            # log_prob: tiene gradientes (red actual)\n",
    "            # old_log_probs: sin gradientes (del buffer)\n",
    "            probability_ratio = torch.exp(log_prob - old_log_probs)\n",
    "\n",
    "            # Fórmula 3.2.2: Clipped Surrogate Objective\n",
    "            # L^CLIP_t(θ) = min(r_t·Â_norm, clip(r_t, 1-ε, 1+ε)·Â_norm)\n",
    "            L_clip = torch.min(\n",
    "                probability_ratio*advatages_norm,\n",
    "                torch.clip(probability_ratio, 1 - clip_epsilon, 1 + clip_epsilon)*advatages_norm\n",
    "            )\n",
    "\n",
    "            # Fórmula 3.3.1: Value Function Loss (MSE contra R_t)\n",
    "            # L^VF_t(φ) = (1/2)·||V_φ(s_t) - R_t||²\n",
    "            return_targets = batch['return_targets']\n",
    "            current_value_pred = agent.critic(state).squeeze(-1)\n",
    "            L_VF = 1/2 * torch.abs(current_value_pred - return_targets)**2\n",
    "\n",
    "            # FASE 4: Total Loss (combinación de Actor y Critic)\n",
    "            # L(θ,φ) = E[-L^CLIP - c2·S[π_θ] + c1·L^VF]\n",
    "            # Invertimos signos: queremos maximizar CLIP y Entropy, minimizar VF\n",
    "            loss = torch.mean(-L_clip + c1*L_VF - c2*entropy)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # FASE 4: Gradient Clipping (previene explosión de gradientes)\n",
    "            # ||∇_Θ|| ≤ max_grad_norm\n",
    "            clip_grad_norm_(agent.parameters(), max_norm=max_norm_value)\n",
    "            \n",
    "            # FASE 4: Parameter Update (Gradient Descent)\n",
    "            # Θ ← Θ - η·∇_Θ L(Θ)\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            entropies.append(entropy.mean().item())\n",
    "        \n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_entropy = np.mean(entropies)\n",
    "    avg_reward = np.mean(buffer._rewards.cpu().numpy())\n",
    "\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_entropies.append(avg_entropy)\n",
    "    avg_rewards.append(avg_reward)\n",
    "\n",
    "    print(f\"Iteration {iteration + 1}/{NUM_ITERATIONS}, Loss: {avg_loss:.4f}, Avg Entropy: {avg_entropy:.4f}, Avg Reward: {avg_reward:.4f}\")\n",
    "    buffer.reset()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(avg_losses)\n",
    "plt.title('Average Loss per Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(avg_entropies)\n",
    "plt.title('Average Entropy per Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Entropy')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(avg_rewards)\n",
    "plt.title('Average Reward per Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Reward')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make(ENV_NAME, render_mode='human', gravity=-10.0,\n",
    "               enable_wind=True, wind_power=15.0, turbulence_power=1.5)\n",
    "\n",
    "agent.eval()\n",
    "for episode in range(2):\n",
    "    state, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action, _ = agent.get_action_and_log_prob(torch.as_tensor(state, device=device))\n",
    "            next_state, reward, terminated, truncated, _ = eval_env.step(int(action.item()))\n",
    "            done = terminated or truncated\n",
    "            total_reward += float(reward)\n",
    "            state = next_state\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb1e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
